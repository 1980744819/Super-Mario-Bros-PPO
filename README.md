# Super Mario Bros with PPO

In this repo, we use Proximal Policy Optimization (PPO) to allow our agent to learn how to play Super Mario Bros from raw pixels. 

The hyperparameters that you'll find in the source file are not the optimal hyperparameters, so please play around with them and see if you can find the best ones!

Another thing that you might want to do is separate our the policy network and the value network, rather than how it is currently set up where they share parameters.

My blog post where I explain the code is located at this link:

https://brandinho.github.io/mario-ppo/
